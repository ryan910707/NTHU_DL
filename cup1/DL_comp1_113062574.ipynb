{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DL Competition 1 Report**\n",
    "\n",
    "---\n",
    "\n",
    "**Student Name:** 葛奕宣 \n",
    "**Student ID:** 113062574  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ryanke/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ryanke/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ryanke/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Download required NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Extraction:\n",
    "---\n",
    "我主要使用BS4進行feature extraction，以下是我提取出的feature(並沒有全數用到)\n",
    "* datetime:\n",
    "    * 透過datetime library進行datetime的轉換，並且把string都改成int(ex: wed to 3)，並且要handle沒有datetime文章的case，我只有在testcase中看到，我根據文章內容hard code了一個時間\n",
    "* title\n",
    "    * 從h1'title'取得\n",
    "* article-topics\n",
    "    * 從'footer.article-topics'取得\n",
    "* \\# of images\n",
    "    * 用len()配上find_all取得\n",
    "* article length\n",
    "    * 先取得article text再用len()取得\n",
    "* \\# of links\n",
    "    * 用len()配上find_all取得\n",
    "* channel\n",
    "    * 尋找'data-channel'\n",
    "\n",
    "針對文字我都有進行tolower(), 並且有透過re library去remove punctuation\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd  # Import pandas\n",
    "\n",
    "# Function to remove punctuation from a string\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def extract_article_features(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extract datetime and convert to year, month, day, hour, and weekday (1 for Monday, 7 for Sunday)\n",
    "    time_element = soup.find('time')\n",
    "    if time_element and time_element.has_attr('datetime'):\n",
    "        # Extract datetime, convert to year, month, day, and hour\n",
    "        datetime_str = time_element['datetime']\n",
    "        try:\n",
    "            # Update the format to match 'Wed, 19 Jun 2013 15:04:30 +0000'\n",
    "            article_datetime = datetime.strptime(datetime_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "            article_year = article_datetime.year\n",
    "            article_month = article_datetime.month\n",
    "            article_day = article_datetime.day\n",
    "            article_weekday = article_datetime.isoweekday()  # Monday is 1, Sunday is 7\n",
    "            article_hour = article_datetime.hour\n",
    "        except ValueError:\n",
    "            # Handle case where the datetime format is incorrect or missing\n",
    "            article_year = 2024\n",
    "            article_month = 1\n",
    "            article_day = 9\n",
    "            article_weekday = 4\n",
    "            article_hour = 19\n",
    "    else:\n",
    "        article_year = 2024\n",
    "        article_month = 1\n",
    "        article_day = 9\n",
    "        article_weekday = 4\n",
    "        article_hour = 19\n",
    "\n",
    "    # Extract title, convert to lowercase and remove punctuation\n",
    "    title = soup.find('h1', class_='title').text.strip().lower() if soup.find('h1', class_='title') else None\n",
    "    title = remove_punctuation(title) if title else None\n",
    "\n",
    "    # Extract author, convert to lowercase and remove punctuation\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'}) if article_info else None\n",
    "    if author_name:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info and article_info.span:\n",
    "        author = article_info.span.string\n",
    "    elif article_info and article_info.a:\n",
    "        author = article_info.a.string\n",
    "    else:\n",
    "        author = None\n",
    "    author = author.lower() if author else None\n",
    "    if author and author.startswith('by '):\n",
    "        author = author[3:]\n",
    "    author = remove_punctuation(author) if author else None\n",
    "\n",
    "    # Extract article topics, convert to lowercase and remove punctuation\n",
    "    article_topics = ' '.join([remove_punctuation(topic.text.lower()) for topic in soup.select('footer.article-topics a')])\n",
    "\n",
    "    # Count the number of images\n",
    "    num_images = len(soup.find_all('img'))\n",
    "\n",
    "    # Calculate article length (number of characters in the article)\n",
    "    article_text = ''.join([p.text for p in soup.select('article p')]).lower()  # Convert article text to lowercase\n",
    "    article_length = len(article_text)\n",
    "\n",
    "    # Count the number of links\n",
    "    num_links = len(soup.find_all('a'))\n",
    "\n",
    "    # Extract channel and convert to lowercase\n",
    "    article_channel = soup.find('article').get('data-channel', None)\n",
    "    article_channel = article_channel.lower() if article_channel else None\n",
    "\n",
    "    return {\n",
    "        'year': article_year,\n",
    "        'month': article_month,\n",
    "        'day': article_day,\n",
    "        'weekday': article_weekday,\n",
    "        'hour': int(article_hour) if article_hour is not None else None,\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'topics': article_topics,\n",
    "        'num_images': num_images,\n",
    "        'article_length': article_length,\n",
    "        'num_links': num_links,\n",
    "        'channel': article_channel\n",
    "    }\n",
    "\n",
    "features = []\n",
    "for page in train_data[\"Page content\"]:\n",
    "    features.append(extract_article_features(page))\n",
    "for page in test_data[\"Page content\"]:\n",
    "    features.append(extract_article_features(page))\n",
    "\n",
    "# Convert the list of feature dictionaries into DataFrames\n",
    "combine_df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "* 這邊主要針對文字進行tokenize跟lemmatization，兩者皆是用nltk的內建function\n",
    "* 接著我使用了sklearn的columntransformer，他把轉換過程包起來，方便針對不同model reuse和客製化\n",
    "* 這裡也是drop column的地方，在不斷實驗後決定drop掉num_image, num_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanke/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ryanke/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ryanke/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/ryanke/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizers\n",
    "def tokenizer(text):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "clf_transform = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('author_vect', CountVectorizer(tokenizer=tokenizer, lowercase=False), 'author'),\n",
    "        ('topic_vect', CountVectorizer(tokenizer=tokenizer, lowercase=False), 'topics'),\n",
    "        ('title_vect', CountVectorizer(tokenizer=tokenizer, lowercase=False), 'title'),\n",
    "        ('channel_vect', CountVectorizer(tokenizer=tokenizer, lowercase=False), 'channel')\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "combine_df.drop(columns=['num_images','num_links'], inplace=True)\n",
    "combine_df = clf_transform.fit_transform(combine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = combine_df[:len(train_data)].astype(np.float32)\n",
    "test_df = combine_df[len(train_data):].astype(np.float32)\n",
    "y_train_all = (train_data['Popularity'].values == 1).astype(np.float32)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df, y_train_all, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'max_depth': [None, 10, 20, 30],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "# }\n",
    "\n",
    "# # Cross-validation strategy\n",
    "# cv_strategy = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize the RandomForestClassifier\n",
    "# rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Initialize GridSearchCV\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='roc_auc',\n",
    "#     cv=cv_strategy,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and estimator\n",
    "# best_params = grid_search.best_params_\n",
    "# best_estimator = grid_search.best_estimator_\n",
    "# print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# y_train_pred_proba = best_estimator.predict_proba(X_train)[:, 1]\n",
    "# train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "# print('Train ROC AUC of best estimator:', train_auc)\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# y_valid_pred_proba = best_estimator.predict_proba(X_valid)[:, 1]\n",
    "# valid_auc = roc_auc_score(y_valid, y_valid_pred_proba)\n",
    "# print('Validation ROC AUC of best estimator:', valid_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "* 我使用model ensemble的方式，實作方式是用VotingClassifier\n",
    "* Model的挑選上，我嘗試了四個比較有名的model, random forest, lightgbm, catboost, XGboost\n",
    "* 其中XGboost的分數明顯比其他低，所以我放棄了他(code註解掉)\n",
    "* 在Model tuning的部分，我有用gridsearch, random search去嘗試不同參數的組合，其中learning_rate, n_estimator是我認為最重要的參數，調整後模型分數顯著提高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5818678872078772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_train,rf.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_valid,rf.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# # Cross-validation strategy\n",
    "# cv_strategy = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# # Initialize the LGBMClassifier\n",
    "# lg = LGBMClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'num_leaves': [31, 50, 70],\n",
    "#     'max_depth': [-1, 10, 20, 30],\n",
    "#     'learning_rate': [0.1, 0.05, 0.01],\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "# }\n",
    "\n",
    "# # Initialize RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=lg,\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=50,\n",
    "#     scoring='roc_auc',\n",
    "#     cv=cv_strategy,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# # Fit the random search to the data\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and estimator\n",
    "# best_params = random_search.best_params_\n",
    "# best_estimator = random_search.best_estimator_\n",
    "# print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# # Evaluate on training data\n",
    "# y_train_pred_proba = best_estimator.predict_proba(X_train)[:, 1]\n",
    "# train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "# print('Train ROC AUC of best estimator:', train_auc)\n",
    "\n",
    "# # Evaluate on validation data\n",
    "# y_valid_pred_proba = best_estimator.predict_proba(X_valid)[:, 1]\n",
    "# valid_auc = roc_auc_score(y_valid, y_valid_pred_proba)\n",
    "# print('Validation ROC AUC of best estimator:', valid_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10916, number of negative: 11198\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6792\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 2684\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493624 -> initscore=-0.025506\n",
      "[LightGBM] [Info] Start training from score -0.025506\n",
      "0.666185359946559\n",
      "0.593456859510363\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lg = LGBMClassifier(n_estimators=40, random_state=42, n_jobs=-1,learning_rate=0.05)\n",
    "lg.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_train,lg.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_valid,lg.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6270849720848046\n",
      "0.588237757895569\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cb = CatBoostClassifier(n_estimators=500, random_state=42, verbose=0, learning_rate=0.01)\n",
    "cb.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_train,cb.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_valid,cb.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# # xgb = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# xgb.fit(X_train, y_train) \n",
    "# print(roc_auc_score(y_train,xgb.predict_proba(X_train)[:, 1]))\n",
    "# print(roc_auc_score(y_valid,xgb.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier\n",
    "* 進行training和validation\n",
    "* 原本沒有進行weighting, 但嘗試過後，把分數較高的model提高了比例，準確度上升很多!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10916, number of negative: 11198\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6792\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 2684\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493624 -> initscore=-0.025506\n",
      "[LightGBM] [Info] Start training from score -0.025506\n",
      "0.688523095490734\n",
      "0.5956656633649682\n"
     ]
    }
   ],
   "source": [
    "# combine the model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf), ('lg', lg), ('cb', cb)], voting='soft', weights=[0.01, 1, 0.5])\n",
    "voting_clf.fit(X_train, y_train)\n",
    "print(roc_auc_score(y_train,voting_clf.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(y_valid,voting_clf.predict_proba(X_valid)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "* 最後使用全部資料進行訓練\n",
    "* 進行test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13632, number of negative: 14011\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8068\n",
      "[LightGBM] [Info] Number of data points in the train set: 27643, number of used features: 3219\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493145 -> initscore=-0.027423\n",
      "[LightGBM] [Info] Start training from score -0.027423\n",
      "0.6761680355609258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "lg = LGBMClassifier(n_estimators=40, random_state=42, n_jobs=-1,learning_rate=0.05)\n",
    "cb = CatBoostClassifier(n_estimators=500, random_state=42, verbose=0, learning_rate=0.01)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf), ('lg', lg), ('cb', cb)], voting='soft', weights=[0.1, 1, 0.5])\n",
    "voting_clf.fit(train_df, y_train_all)\n",
    "print(roc_auc_score(y_train_all,voting_clf.predict_proba(train_df)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y = voting_clf.predict_proba(test_df)[:, 1]\n",
    "\n",
    "df_pred = pd.DataFrame({'Id': test_data['Id'], 'Popularity': final_y})\n",
    "df_pred.to_csv('prediciton.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "* 這次作業讓我意識到原來Feature Extraction和Model tuning竟然能讓模型的分數差這麼多\n",
    "* 我認為最大的pitfall就是使用大量的段落文字，我嘗試過第一段、全部文章，效果都非常差，我想可能是資料的數量不夠多，所以文字跟結果的關聯性非常低\n",
    "* 學習到了bs4這個強大的HTML提取package，以及re處理文字的package，兩者結合起來讓我對分析網站內容更加熟悉\n",
    "* 上網學到了model ensemble的技術，在資源足夠的情況下結合多個model確實能有幫助\n",
    "* 接觸到了不同的classifier, 其中LightGBM最令我驚豔，速度快很多，準確度也是最高的\n",
    "* 最後是我原本以為votingclassifier若傳入已經train過的模型，會直接使用，上網查後原來都會retrain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
